{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I need to import the dataset, and to do this I need to format the data a little \n",
    "src: Udacity, June 2021, mnist_mlp_exercise.ipynb, source code, https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229 \n",
    "    NOTE: The code cited above was left partially blank and was also filled in with my own work, but as I can no longer differentiate what I worked on I am citing the whole file\n",
    "dataset source: Cohen, G., Afshar, S., Tapson, J., & van Schaik, A. (2017). EMNIST: an extension of MNIST to handwritten letters. Retrieved from http://arxiv.org/abs/1702.05373\n",
    "    NOTE: This is the official citation for the EMNIST dataset, though it's call is built into torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 20\n",
    "#Creates a transform for the data that will make a tensor of the image data\n",
    "tensorform = transforms.ToTensor()\n",
    "#Sets up the groups of data\n",
    "trainingData = datasets.EMNIST(root='dataset', train='True', split='balanced', download='True', transform=tensorform)\n",
    "testingData = datasets.EMNIST(root='dataset', train='False', split='balanced', download='True', transform=tensorform)\n",
    "#Loads them to be used for training purposes\n",
    "trainingLoad = torch.utils.data.DataLoader(trainingData, batch_size=batch, num_workers=0, shuffle=True, drop_last=True)\n",
    "testingLoad = torch.utils.data.DataLoader(testingData, batch_size=batch, num_workers=0, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "    \n",
    "# obtain one batch of training images\n",
    "dataiter = iter(trainingLoad)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy()\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "    # print out the correct label for each image\n",
    "    # .item() gets the value contained in a Tensor\n",
    "    ax.set_title(str(labels[idx].item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Designes the neural network that will be used to train our ml model.\n",
    "Since I will be needing an output of 26 letters, for both capital and lower case, and 10 digits, I will need a total of 62 outputs, if I decrease the linear level by a factor of 2 accross 4 layers, we will get a starting number of layers of 992, followed by 496, 248, and  124."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class myNN(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(myNN, self).__init__()\n",
    "#         self.forward1 = nn.Linear(784, 512)\n",
    "#         self.forward2 = nn.Linear(512, 256)\n",
    "#         self.forward3 = nn.Linear(256, 256)\n",
    "#         self.forward35 = nn.Linear(256, 128)\n",
    "#         self.forward4 = nn.Linear(128, 47)\n",
    "#         #sets a dropout rate for the individual nodes of 15%\n",
    "#         self.dropout = nn.Dropout(p=.15)\n",
    "\n",
    "#     def forward(self, element):\n",
    "#         element = element.view(-1, 784)\n",
    "#         element = self.dropout(fun.relu(self.forward1(element)))\n",
    "#         element = self.dropout(fun.relu(self.forward2(element)))\n",
    "#         element = self.dropout(fun.relu(self.forward3(element)))\n",
    "#         element = self.dropout(fun.relu(self.forward35(element)))\n",
    "#         element = self.forward4(element)\n",
    "#         return element\n",
    "from MyNN import myNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will need to initialize the neural network model and choose the loss function that will be used, as well as choose the optimization function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = myNN()\n",
    "lossfun = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually training the network:\n",
    "    I need a loop that goes through a number of iterations that will be defined beforehand \n",
    "    Need to track the efficiency of the training, so I will need to also test each iteration\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration  0 \tTraining loss: 2.314045\n",
      "Iteration  1 \tTraining loss: 0.876970\n",
      "Iteration  2 \tTraining loss: 0.688836\n",
      "Iteration  3 \tTraining loss: 0.599636\n",
      "Iteration  4 \tTraining loss: 0.547879\n",
      "Iteration  5 \tTraining loss: 0.508455\n",
      "Iteration  6 \tTraining loss: 0.479222\n",
      "Iteration  7 \tTraining loss: 0.456705\n",
      "Iteration  8 \tTraining loss: 0.437283\n",
      "Iteration  9 \tTraining loss: 0.419537\n"
     ]
    }
   ],
   "source": [
    "totalIterations = 10\n",
    "#set the model to training mode\n",
    "model.train()\n",
    "\n",
    "for iteration in range(totalIterations):\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for data, target in trainingLoad:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        #figure out the loss\n",
    "        loss = lossfun(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * data.size(0)\n",
    "    train_loss = train_loss/len(trainingLoad.dataset)\n",
    "\n",
    "    #prints the information about how well training is going\n",
    "    print(\"Iteration \", iteration, \"\\tTraining loss: {:.6f}\".format(train_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for __, data in trainingData:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests the formerly trained model using the test set that was created initially "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.192353\n",
      "\n",
      "Test accuracy of\t 0 :  74.79166666666667 % ( 1795.0 / 2400.0 )\n",
      "Test accuracy of\t 1 :  81.29166666666667 % ( 1951.0 / 2400.0 )\n",
      "Test accuracy of\t 2 :  98.5 % ( 2364.0 / 2400.0 )\n",
      "Test accuracy of\t 3 :  99.45833333333333 % ( 2387.0 / 2400.0 )\n",
      "Test accuracy of\t 4 :  98.20833333333333 % ( 2357.0 / 2400.0 )\n",
      "Test accuracy of\t 5 :  91.33333333333333 % ( 2192.0 / 2400.0 )\n",
      "Test accuracy of\t 6 :  97.20833333333333 % ( 2333.0 / 2400.0 )\n",
      "Test accuracy of\t 7 :  99.5 % ( 2388.0 / 2400.0 )\n",
      "Test accuracy of\t 8 :  98.66666666666667 % ( 2368.0 / 2400.0 )\n",
      "Test accuracy of\t 9 :  91.58333333333333 % ( 2198.0 / 2400.0 )\n",
      "Test accuracy of\t 10 :  99.54166666666667 % ( 2389.0 / 2400.0 )\n",
      "Test accuracy of\t 11 :  98.04166666666667 % ( 2353.0 / 2400.0 )\n",
      "Test accuracy of\t 12 :  98.95833333333333 % ( 2375.0 / 2400.0 )\n",
      "Test accuracy of\t 13 :  97.33333333333333 % ( 2336.0 / 2400.0 )\n",
      "Test accuracy of\t 14 :  99.54166666666667 % ( 2389.0 / 2400.0 )\n",
      "Test accuracy of\t 15 :  65.75 % ( 1578.0 / 2400.0 )\n",
      "Test accuracy of\t 16 :  97.375 % ( 2337.0 / 2400.0 )\n",
      "Test accuracy of\t 17 :  99.25 % ( 2382.0 / 2400.0 )\n",
      "Test accuracy of\t 18 :  70.25 % ( 1686.0 / 2400.0 )\n",
      "Test accuracy of\t 19 :  98.25 % ( 2358.0 / 2400.0 )\n",
      "Test accuracy of\t 20 :  99.25 % ( 2382.0 / 2400.0 )\n",
      "Test accuracy of\t 21 :  44.708333333333336 % ( 1073.0 / 2400.0 )\n",
      "Test accuracy of\t 22 :  99.33333333333333 % ( 2384.0 / 2400.0 )\n",
      "Test accuracy of\t 23 :  99.29166666666667 % ( 2383.0 / 2400.0 )\n",
      "Test accuracy of\t 24 :  66.54166666666667 % ( 1597.0 / 2400.0 )\n",
      "Test accuracy of\t 25 :  99.54166666666667 % ( 2389.0 / 2400.0 )\n",
      "Test accuracy of\t 26 :  99.04166666666667 % ( 2377.0 / 2400.0 )\n",
      "Test accuracy of\t 27 :  99.375 % ( 2385.0 / 2400.0 )\n",
      "Test accuracy of\t 28 :  97.625 % ( 2343.0 / 2400.0 )\n",
      "Test accuracy of\t 29 :  97.79166666666667 % ( 2347.0 / 2400.0 )\n",
      "Test accuracy of\t 30 :  98.54166666666667 % ( 2365.0 / 2400.0 )\n",
      "Test accuracy of\t 31 :  96.66666666666667 % ( 2320.0 / 2400.0 )\n",
      "Test accuracy of\t 32 :  99.375 % ( 2385.0 / 2400.0 )\n",
      "Test accuracy of\t 33 :  99.45833333333333 % ( 2387.0 / 2400.0 )\n",
      "Test accuracy of\t 34 :  94.91666666666667 % ( 2278.0 / 2400.0 )\n",
      "Test accuracy of\t 35 :  88.125 % ( 2115.0 / 2400.0 )\n",
      "Test accuracy of\t 36 :  97.08333333333333 % ( 2330.0 / 2400.0 )\n",
      "Test accuracy of\t 37 :  96.04166666666667 % ( 2305.0 / 2400.0 )\n",
      "Test accuracy of\t 38 :  99.08333333333333 % ( 2378.0 / 2400.0 )\n",
      "Test accuracy of\t 39 :  98.66666666666667 % ( 2368.0 / 2400.0 )\n",
      "Test accuracy of\t 40 :  81.125 % ( 1947.0 / 2400.0 )\n",
      "Test accuracy of\t 41 :  74.58333333333333 % ( 1790.0 / 2400.0 )\n",
      "Test accuracy of\t 42 :  98.91666666666667 % ( 2374.0 / 2400.0 )\n",
      "Test accuracy of\t 43 :  96.33333333333333 % ( 2312.0 / 2400.0 )\n",
      "Test accuracy of\t 44 :  71.125 % ( 1707.0 / 2400.0 )\n",
      "Test accuracy of\t 45 :  97.25 % ( 2334.0 / 2400.0 )\n",
      "Test accuracy of\t 46 :  95.33333333333333 % ( 2288.0 / 2400.0 )\n"
     ]
    }
   ],
   "source": [
    "tloss = 0.0\n",
    "groupCorrect = list(0. for i in range(62))\n",
    "groupTotal = list(0. for i in range(62))\n",
    "#set the model to evaluation mode so that it doesn't try to alter it, also speeds up the testing\n",
    "model.eval()\n",
    "\n",
    "for data, target in testingLoad:\n",
    "    out = model(data)\n",
    "    loss = lossfun(out, target)\n",
    "    #update the test's loss\n",
    "    tloss += loss.item() * data.size(0)\n",
    "    #converts the output to a prediction\n",
    "    _, prediction = torch.max(out, 1)\n",
    "    correct = np.squeeze(prediction.eq(target.data.view_as(prediction)))\n",
    "    for i in range(batch):\n",
    "        label = target.data[i] \n",
    "        groupCorrect[label]  += correct[i].item()\n",
    "        groupTotal[label] += 1\n",
    "#I anticipate some higher loss in testing as the model will not be used to this dataset\n",
    "tloss = tloss/len(testingLoad.dataset)\n",
    "print('Test loss: {:.6f}\\n'.format(tloss))\n",
    "\n",
    "for i in range(62):\n",
    "    if groupTotal[i] > 0:\n",
    "        print(\"Test accuracy of\\t\", i, \": \", (100 * groupCorrect[i] / groupTotal[i]), \"% (\", groupCorrect[i], \"/\", groupTotal[i], \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section is for saving the model for use in other scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'nnmodel.pth')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d6fd6338a7249ab3f85ae7dcb056fe002d53a2ffb3a9e1f850a66f8cf83b0189"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
